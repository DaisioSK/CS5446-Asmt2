{"cells":[{"cell_type":"markdown","id":"ea5fecf5210ad656","metadata":{"id":"ea5fecf5210ad656"},"source":["# Assignment 2: Deep Reinforcement Learning"]},{"cell_type":"markdown","id":"b4e6c201-eb6f-491d-9430-e660fad80eaf","metadata":{"id":"b4e6c201-eb6f-491d-9430-e660fad80eaf"},"source":["## Overview\n","\n","In this assignment, we will explore how to tackle sequential decision-making problems using reinforcement learning, without prior knowledge of the environment's dynamics. Specifically, we will try to learn to solve an instance of the Elevator environment. The Elevator environment models evening rush hours when people from different floors in a building want to go down to the bottom floor using elevators. Our goal is to develop an optimal policy for controlling the elevators to transport people efficiently to the ground floor.\n","\n","We will employ deep reinforcement learning to discover the optimal policy, specifically using the Actor-Critic algorithm combined with Proximal Policy Optimization (PPO). Unlike standard policy gradient methods, the Actor-Critic framework learns both a policy (the actor) and a value function (the critic) concurrently. The critic enhances the efficiency of the policy improvement process by providing better bootstrap returns. To stabilize learning, we will utilize PPO for our objective function, which simplifies the concepts introduced in Trust Region Policy Optimization (TRPO) where it optimizes a surrogate objective while constraining policy updates with a KL divergence term. PPO employs a clipped surrogate objective to ensure that policy updates do not deviate excessively from the current policy [1],achieving results similar to those of TRPO while being much simpler to implement.\n","\n","Throughout this assignment, you will build an Actor-Critic agent. You will implement both actor and critic networks using deep neural networks, deriving a stochastic policy from the actor network. Additionally, you will learn to implement key components of the Actor-Critic PPO training process, including calculating advantages and computing the PPO loss. This will involve defining the policy objective, value functions, and entropy components.\n","\n","[1] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347. <https://arxiv.org/abs/1707.06347>."]},{"cell_type":"markdown","id":"fc9f3610-cb46-46d2-98ea-1c0eb4bfdc42","metadata":{"id":"fc9f3610-cb46-46d2-98ea-1c0eb4bfdc42"},"source":["## Setup"]},{"cell_type":"markdown","id":"7d03007a-33cd-48d8-8fef-77ce70bb7b3c","metadata":{"id":"7d03007a-33cd-48d8-8fef-77ce70bb7b3c"},"source":["### Installing Dependencies\n","\n","The elevator task is implemented using the `PyRDDLGym` library. Before we begin, please install the following packages."]},{"cell_type":"code","execution_count":null,"id":"dcRL2fcfs3m0","metadata":{"id":"dcRL2fcfs3m0"},"outputs":[],"source":["!pip install pyRDDLGym\n","!pip install rddlrepository"]},{"cell_type":"markdown","id":"16389962-52e5-44de-a0be-11cab1a2807a","metadata":{"id":"16389962-52e5-44de-a0be-11cab1a2807a"},"source":["### Using Google Colab\n","\n","If you are using Google Colab (and we encourage you to do so), please run the following code cell. If you are not using Google Colab, you can skip this code cell.\n","\n","**Note**: The path `'/content/drive/'` cannot be changed. For example, if your assignment folder in Google Drive is located at `My Drive -> CSXX46A2`, you should specify the path as `'/content/drive/MyDrive/CSXX46A2'`."]},{"cell_type":"code","execution_count":null,"id":"4424c383-ca3b-4ae8-9885-35df6323c9b5","metadata":{"id":"4424c383-ca3b-4ae8-9885-35df6323c9b5"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/CSXX46A2')\n","\n","%cd /content/drive/MyDrive/CSXX46A2"]},{"cell_type":"markdown","id":"0b532d64-fbbc-443b-9d75-b60473fc9955","metadata":{"id":"0b532d64-fbbc-443b-9d75-b60473fc9955"},"source":["## Import Dependencies"]},{"cell_type":"code","execution_count":null,"id":"3077faa38d137d25","metadata":{"ExecuteTime":{"end_time":"2024-10-16T01:34:25.080430Z","start_time":"2024-10-16T01:34:17.110272Z"},"id":"3077faa38d137d25"},"outputs":[],"source":["import pyRDDLGym\n","from pyRDDLGym.core.env import RDDLEnv\n","\n","from utils import DictToListWrapper, live_plot\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import random\n","import tqdm\n","\n","from torch.distributions.categorical import Categorical\n","\n","import gymnasium as gym\n","from gymnasium.wrappers import RecordEpisodeStatistics\n","from gymnasium.vector import SyncVectorEnv"]},{"cell_type":"markdown","id":"19857e45-be05-430e-bf4f-a384222053fb","metadata":{"id":"19857e45-be05-430e-bf4f-a384222053fb"},"source":["## Environment"]},{"cell_type":"markdown","id":"54ea86b9-2216-4878-936c-c3b05cd79cfc","metadata":{"id":"54ea86b9-2216-4878-936c-c3b05cd79cfc"},"source":["To ease environment creation in the subsequent steps, we define a function that returns an instance of the environment below. We will also include the `DictToListWrapper`, as we did in the `ElevatorEnv` notebook. Additionally, we will incorporate the `RecordEpisodeStatistics` wrapper to help track episode statistics."]},{"cell_type":"code","execution_count":null,"id":"e8f47524-7bf9-41e9-b615-e7274024e33f","metadata":{"id":"e8f47524-7bf9-41e9-b615-e7274024e33f"},"outputs":[],"source":["def create_elevator_env():\n","    env = RDDLEnv(\n","        domain=\"selfDefinedEnvs/domain.rddl\",\n","        instance=\"selfDefinedEnvs/instance5.rddl\",  # instance-5 file\n","    )\n","    # If your observation is a Dict of booleans, flatten it:\n","    env = DictToListWrapper(env)\n","    env = RecordEpisodeStatistics(env)\n","    return env"]},{"cell_type":"markdown","id":"55f4e788b1ca427f","metadata":{"id":"55f4e788b1ca427f"},"source":["### Vectorized Environments\n","\n","To improve training efficiency and reduce the correlation between samples in a single sequence of experiences, we utilize multiple parallel environments to collect data. At each step, the agent interacts with $N$ environments simultaneously, storing the collected transitions in a rollout buffer. These transitions are then used to update the policy and value function.\n","\n","We will use the `SyncVectorEnv` class to create multiple environments. Here, we set the number of parallel environments to 4.\n","\n","**DO NOT MODIFY THE CODE BELOW**"]},{"cell_type":"code","execution_count":null,"id":"3bd51f7cb092701b","metadata":{"ExecuteTime":{"end_time":"2024-10-16T01:34:36.895887Z","start_time":"2024-10-16T01:34:27.277299Z"},"id":"3bd51f7cb092701b"},"outputs":[],"source":["NUM_ENVS = 4\n","\n","envs = SyncVectorEnv([lambda: create_elevator_env() for _ in range(NUM_ENVS)])"]},{"cell_type":"markdown","id":"0e1b3f02-af8d-41c1-b4b2-512275372387","metadata":{"id":"0e1b3f02-af8d-41c1-b4b2-512275372387"},"source":["Our vectorized environments have identical observation and action spaces, as shown below."]},{"cell_type":"code","execution_count":null,"id":"ac824ae0-7cc8-403b-97d6-eb16822c0b5f","metadata":{"id":"ac824ae0-7cc8-403b-97d6-eb16822c0b5f"},"outputs":[],"source":["env = envs.envs[0].env\n","print(f\"Observation space: {env.observation_space}\")\n","env.get_state_description()\n","\n","print(f\"Action space: {env.action_space}\")\n","env.get_action_description()"]},{"cell_type":"markdown","id":"9b5b4655222d7266","metadata":{"id":"9b5b4655222d7266"},"source":["## Hyperparameters\n","\n","Here, we define the hyperparameters for the algorithm. The random seed is fixed to ensure reproducibility.\n","\n","**DO NOT MODIFY THE CODE BELOW**"]},{"cell_type":"code","execution_count":null,"id":"a7f069423e1dea92","metadata":{"ExecuteTime":{"end_time":"2024-10-16T01:34:36.983978Z","start_time":"2024-10-16T01:34:36.896890Z"},"id":"a7f069423e1dea92"},"outputs":[],"source":["LEARNING_RATE = 2.5e-4\n","\n","ROLLOUT_STEPS = 128\n","NUM_MINI_BATCHES = NUM_EPOCHS = 4\n","TOTAL_STEPS = 800000\n","\n","GAMMA = 0.99\n","GAE_LAMBDA = 0.95\n","\n","CLIP_COEF = 0.2\n","VALUE_LOSS_COEF = 0.5\n","ENTROPY_COEF = 0.01\n","\n","# RANDOM SEED, DON'T MODIFY\n","SEED = 2048\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","id":"0335e9c9-6e29-47c9-8181-b244fb892d3c","metadata":{"id":"0335e9c9-6e29-47c9-8181-b244fb892d3c"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"id":"6a642351-9744-4d23-a98a-d654fa02dbc9","metadata":{"id":"6a642351-9744-4d23-a98a-d654fa02dbc9"},"outputs":[],"source":["def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n","    \"\"\"Initialize the weights and biases of a layer.\n","\n","    Args:\n","        layer (nn.Module): The layer to initialize.\n","        std (float): Standard deviation for orthogonal initialization.\n","        bias_const (float): Constant value for bias initialization.\n","\n","    Returns:\"\n","        nn.Module: The initialized layer.\n","    \"\"\"\n","    torch.nn.init.orthogonal_(layer.weight, std)  # Orthogonal initialization\n","    torch.nn.init.constant_(layer.bias, bias_const)  # Constant bias\n","    return layer"]},{"cell_type":"markdown","id":"daed3a0e88009163","metadata":{"id":"daed3a0e88009163"},"source":["## Actor-Critic Agent\n","\n","The Actor-Critic agent is a type of reinforcement learning algorithm that combines two fundamental components: the **actor** and the **critic**.\n","\n","- **Actor**: This part of the agent is responsible for determining which actions to take in a given state, effectively defining the policy. It maps states to action probabilities, allowing the agent to explore different actions based on learned strategies.\n","\n","- **Critic**: The critic evaluates the action taken by the actor by estimating the value function, which represents the expected return (or future rewards) for a given state. This feedback helps the actor improve its policy over time.\n","\n","By integrating both components, the Actor-Critic method benefits from the strengths of policy-based and value-based approaches, enabling more efficient learning in complex environments.\n","\n","### Task 1: Implement the Actor-Critic Agent\n","\n","In this task, you will complete a partially defined actor-critic agent template. Your implementation will involve the following components:\n","\n","#### Task 1.1: Actor and Critic Networks\n","\n","The actor-critic agent consists of two neural networks:\n","\n","- **Actor Network**: This approximates the policy, mapping states to action probabilities (logits).\n","- **Critic Network**: This approximates the value function, mapping states to their corresponding value estimates.\n","\n","You will need to specify the input and output dimensions for both the actor and critic networks:\n","\n","- For the actor, define `actor_input_dim` and `actor_output_dim`.\n","- For the critic, define `critic_input_dim` and `critic_output_dim`.\n","\n","These dimensions should be determined based on the specifics of your environment.\n","\n","#### Task 1.2: Get Value\n","\n","Implement the `get_value` function, which should return the values of given states by passing it through the critic network.\n","\n","#### Task 1.3: Get Action Probabilities\n","\n","Implement the `get_probs` function. This function should return the probability distribution over possible actions for a given state by using the actor network to compute logits. Since actions are discrete, you will use a categorical distribution for this.\n","\n","**Hint**: Utilize the [`Categorical`](https://pytorch.org/docs/stable/distributions.html#categorical) class from `torch.distributions.categorical`.\n","\n","#### Task 1.4: Get Action\n","\n","Implement the `get_action` function to return an action sampled from the action probabilities obtained in the previous step. Remember, the actor-critic agent employs a stochastic policy.\n","\n","#### Task 1.5: Get Log Probability of a Given Action\n","\n","Implement the `get_action_logprob` function to return the log probability of a specified action based on the action probabilities generated earlier."]},{"cell_type":"code","execution_count":null,"id":"b45d22f7fda1f24d","metadata":{"ExecuteTime":{"end_time":"2024-10-16T01:34:38.466418Z","start_time":"2024-10-16T01:34:37.581226Z"},"id":"b45d22f7fda1f24d"},"outputs":[],"source":["class ACAgent(nn.Module):\n","    \"\"\"Actor-Critic agent using neural networks for policy and value function approximation.\"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Initialize the Actor-Critic agent with actor and critic networks.\"\"\"\n","        super().__init__()\n","\n","        ### ------------- TASK 1.1 ----------- ###\n","        ### ----- YOUR CODES START HERE ------ ###\n","        actor_input_dim = ?  # Input dimension for the actor\n","        actor_output_dim = ?  # Output dimension for the actor (number of actions)\n","        critic_input_dim = ?  # Input dimension for the critic\n","        critic_output_dim = ?  # Output dimension for the critic (value estimate)\n","        ### ------ YOUR CODES END HERE ------- ###\n","\n","        # Define the actor network\n","        self.actor = nn.Sequential(\n","            layer_init(nn.Linear(actor_input_dim, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 128)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(128, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, actor_output_dim), std=0.01),  # Final layer with small std for output\n","        )\n","\n","        # Define the critic network\n","        self.critic = nn.Sequential(\n","            layer_init(nn.Linear(critic_input_dim, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 128)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(128, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, critic_output_dim), std=1.0),  # Standard output layer for value\n","        )\n","\n","    def get_value(self, x):\n","        \"\"\"Calculate the estimated value for a given state.\n","\n","        Args:\n","            x (torch.Tensor): Input state, shape: (batch_size, observation_size)\n","\n","        Returns:\n","            torch.Tensor: Estimated value for the state, shape: (batch_size, 1)\n","        \"\"\"\n","        ### ------------- TASK 1.2 ----------- ###\n","        ### ----- YOUR CODES START HERE ------ ###\n","        value = ?  # Forward pass through the critic network\n","        ### ------ YOUR CODES END HERE ------- ###\n","        return value\n","\n","    def get_probs(self, x):\n","        \"\"\"Calculate the action probabilities for a given state.\n","\n","        Args:\n","            x (torch.Tensor): Input state, shape: (batch_size, observation_size)\n","\n","        Returns:\n","            torch.distributions.Categorical: Categorical distribution over actions.\n","        \"\"\"\n","        ### ------------- TASK 1.3 ----------- ###\n","        ### ----- YOUR CODES START HERE ------ ###\n","        logits = ?  # Get logits from the actor network\n","        probs = ?  # Create a categorical distribution from the logits\n","        ### ------ YOUR CODES END HERE ------- ###\n","        return probs\n","\n","    def get_action(self, probs):\n","        \"\"\"Sample an action from the action probabilities.\n","\n","        Args:\n","            probs (torch.distributions.Categorical): Action probabilities.\n","\n","        Returns:\n","            torch.Tensor: Sampled action, shape: (batch_size, 1)\n","        \"\"\"\n","        ### ------------- TASK 1.4 ----------- ###\n","        ### ----- YOUR CODES START HERE ------ ###\n","        action = ?  # Sample an action based on the probabilities\n","        ### ------ YOUR CODES END HERE ------- ###\n","        return action\n","\n","    def get_action_logprob(self, probs, action):\n","        \"\"\"Compute the log probability of a given action.\n","\n","        Args:\n","            probs (torch.distributions.Categorical): Action probabilities.\n","            action (torch.Tensor): Selected action, shape: (batch_size, 1)\n","\n","        Returns:\n","            torch.Tensor: Log probability of the action, shape: (batch_size, 1)\n","        \"\"\"\n","        ### ------------- TASK 1.5 ----------- ###\n","        ### ----- YOUR CODES START HERE ------ ###\n","        logprob = ?  # Calculate log probability of the sampled action\n","        ### ------ YOUR CODES END HERE ------- ###\n","        return logprob\n","\n","    def get_entropy(self, probs):\n","        \"\"\"Calculate the entropy of the action distribution.\n","\n","        Args:\n","            probs (torch.distributions.Categorical): Action probabilities.\n","\n","        Returns:\n","            torch.Tensor: Entropy of the distribution, shape: (batch_size, 1)\n","        \"\"\"\n","        return probs.entropy()  # Return the entropy of the probabilities\n","\n","    def get_action_logprob_entropy(self, x):\n","        \"\"\"Get action, log probability, and entropy for a given state.\n","\n","        Args:\n","            x (torch.Tensor): Input state.\n","\n","        Returns:\n","            tuple: (action, logprob, entropy)\n","                - action (torch.Tensor): Sampled action.\n","                - logprob (torch.Tensor): Log probability of the action.\n","                - entropy (torch.Tensor): Entropy of the action distribution.\n","        \"\"\"\n","        probs = self.get_probs(x)  # Get the action probabilities\n","        action = self.get_action(probs)  # Sample an action\n","        logprob = self.get_action_logprob(probs, action)  # Compute log probability of the action\n","        entropy = self.get_entropy(probs)  # Compute entropy of the action distribution\n","        return action, logprob, entropy  # Return action, log probability, and entropy"]},{"cell_type":"markdown","id":"0a2793bc-fb69-419e-9a76-96dabb6d9a2c","metadata":{"id":"0a2793bc-fb69-419e-9a76-96dabb6d9a2c"},"source":["We can initialize our agent as follows."]},{"cell_type":"code","execution_count":null,"id":"c419b850-75ba-49f2-bd57-95af020320a1","metadata":{"id":"c419b850-75ba-49f2-bd57-95af020320a1"},"outputs":[],"source":["agent = ACAgent().to(device)"]},{"cell_type":"markdown","id":"8d3fe2c2-4f5a-481d-897a-a37ebe95ec5b","metadata":{"id":"8d3fe2c2-4f5a-481d-897a-a37ebe95ec5b"},"source":["We can do a sanity check for the agent implementation by executing the code below."]},{"cell_type":"code","execution_count":null,"id":"968efb08-a175-4226-81af-966b32398acd","metadata":{"id":"968efb08-a175-4226-81af-966b32398acd"},"outputs":[],"source":["test_x = torch.zeros(10, envs.single_observation_space.shape[0], device=device)\n","test_probs = Categorical(F.one_hot(torch.arange(0, envs.single_action_space.n, device=device), num_classes=envs.single_action_space.n))\n","test_actions = torch.tensor([0, 1, 2, 3, 4, 5], device=device)\n","test_logprob = torch.tensor([-1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07], device=device)\n","\n","assert list(agent.get_value(test_x).shape) == [10, 1]\n","assert list(agent.get_probs(test_x).logits.shape) == [10, envs.single_action_space.n]\n","assert (agent.get_action(test_probs) == test_actions).all()\n","assert torch.allclose(agent.get_action_logprob(test_probs, test_actions), test_logprob)"]},{"cell_type":"markdown","id":"b3f38596-948b-40d4-a5d8-5e24f5f86705","metadata":{"id":"b3f38596-948b-40d4-a5d8-5e24f5f86705"},"source":["## Actor-Critic Training with Proximal Policy Optimization (PPO)\n","\n","Actor-critic training with Proximal Policy Optimization (PPO) is a powerful reinforcement learning framework that enhances both the stability and efficiency of policy learning.\n","\n","### Training Process\n","\n","1. **Rollout**: The agent interacts with the environment for a predefined number of steps, known as the rollout. During this phase, the actor selects actions based on its current policy, while the agent gathers states, actions, rewards, and other relevant information, which are stored in a rollout buffer. This collected data forms the basis for subsequent policy and value updates.\n","\n","2. **Advantage Estimation**: After completing the rollout, the agent computes advantages using the Generalized Advantage Estimation (GAE) method. This technique produces stable and low-variance estimates of the advantage function, indicating how much better or worse an action performed compared to the expected return. GAE improves learning efficiency by providing more accurate feedback for policy updates.\n","\n","3. **Updating Actor and Critic**:\n","\n","   - **Policy Update**: The actor's policy is refined using the computed advantages. PPO employs a clipped surrogate objective to limit the magnitude of policy updates, ensuring that changes remain within a safe range. This clipping mechanism fosters stable learning and reduces the risk of drastic performance drops.\n","\n","   - **Value Update**: The critic's value function is updated to minimize the difference between predicted values and actual returns, typically using mean squared error (MSE) loss. This adjustment allows the critic to provide reliable feedback to the actor, enhancing the overall learning process.\n","\n","   - **Entropy Regularization**: To promote exploration, PPO includes an entropy term in its objective function. This term discourages certainty in action selection, encouraging the agent to explore a wider range of actions and preventing premature convergence on suboptimal policies."]},{"cell_type":"markdown","id":"4fa465ec-5cb4-46bf-af3f-4586e94fa312","metadata":{"id":"4fa465ec-5cb4-46bf-af3f-4586e94fa312"},"source":["### Rollout"]},{"cell_type":"markdown","id":"615ecfe1a1181237","metadata":{"id":"615ecfe1a1181237"},"source":["#### Rollout Buffer\n","\n","In the rollout buffer, we gather experiences from the environment over a defined number of steps, known as `ROLLOUT_STEPS`. These experiences are then used to update the agent’s policy and value function. Once the update is complete, the experiences in the buffer are discarded. This method contrasts with the DQN algorithm, which retains and reuses transitions from a replay buffer for multiple updates.\n","\n","The rollout buffer captures a variety of information essential for updating the agent, including:\n","\n","- **States**: The states encountered by the agent.\n","- **Actions**: The actions taken by the agent in each state.\n","- **Rewards**: The rewards received after taking those actions.\n","- **Done Flags**: Indicators of whether an episode has ended.\n","- **Log Probabilities**: The log probabilities of the actions taken, as determined by the actor.\n","- **Values**: The value estimates for the states from the critic.\n","\n","Given that we have `NUM_ENVS` parallel environments, the shape of the rollout buffer will be structured as `(ROLLOUT_STEPS, NUM_ENVS)`. This allows us to efficiently gather and store data from multiple environments simultaneously, facilitating more robust training and improved sample efficiency."]},{"cell_type":"code","execution_count":null,"id":"9557828f920b5818","metadata":{"ExecuteTime":{"end_time":"2024-10-16T01:34:39.718910Z","start_time":"2024-10-16T01:34:39.711927Z"},"id":"9557828f920b5818"},"outputs":[],"source":["states = torch.zeros((ROLLOUT_STEPS, NUM_ENVS) + envs.single_observation_space.shape).to(device)\n","actions = torch.zeros((ROLLOUT_STEPS, NUM_ENVS) + envs.single_action_space.shape).to(device)\n","rewards = torch.zeros((ROLLOUT_STEPS, NUM_ENVS)).to(device)\n","dones = torch.zeros((ROLLOUT_STEPS, NUM_ENVS)).to(device)\n","\n","logprobs = torch.zeros((ROLLOUT_STEPS, NUM_ENVS)).to(device)\n","values = torch.zeros((ROLLOUT_STEPS, NUM_ENVS)).to(device)"]},{"cell_type":"markdown","id":"d3ab00f1","metadata":{"id":"d3ab00f1"},"source":["#### Reward Normalization\n","\n","To stabilize and improve the training process in reinforcement learning, we sometimes adjust and scales the rewards received by the agent to ensure that they remain within a consistent range. Common methods for reward normalization include standardization (subtracting the mean and dividing by the standard deviation) or min-max scaling (rescaling rewards to a specified range).\n","\n","Here, we compute the minimum and maximum rewards of the environment, which we will later use for min-max scaling of the rewards."]},{"cell_type":"code","execution_count":null,"id":"df46a9db","metadata":{"id":"df46a9db"},"outputs":[],"source":["floors = 5\n","max_waiting = 3\n","max_in_ele = 10\n","in_ele_penalty = 0.75\n","people_waiting_penalty = 3.0\n","reward_delivered = 30\n","\n","min_reward = - in_ele_penalty * max_in_ele - people_waiting_penalty * max_waiting * floors\n","max_reward = max_in_ele * reward_delivered\n","\n","print(\"Minimum reward:\", min_reward)\n","print(\"Maximum reward:\", max_reward)"]},{"cell_type":"markdown","id":"cdecd56f-706f-4146-89bb-345c0404d44f","metadata":{"id":"cdecd56f-706f-4146-89bb-345c0404d44f"},"source":["#### Batch, Mini Batch, and Iterations\n","\n","After completing a rollout of length $T$ across $N$ parallel environments, we obtain one batch of data with a size of $T \\times N$. This total represents the batch size. We train the actor-critic agent using this batch by splitting the data into minibatches and training for $E$ epochs.\n","\n","The total number of iterations or updates is calculated by dividing the total number of interaction steps by the batch size.\n","\n","**Note:** The batch size mentioned here refers specifically to the rollout batch size. In the code implementation, we use `batch_size` to denote the number of batched inputs/outputs/data, which may differ from the rollout batch size."]},{"cell_type":"code","execution_count":null,"id":"20a33cfd38b4526f","metadata":{"ExecuteTime":{"end_time":"2024-10-16T01:34:41.026894Z","start_time":"2024-10-16T01:34:41.023762Z"},"id":"20a33cfd38b4526f"},"outputs":[],"source":["BATCH_SIZE = ROLLOUT_STEPS * NUM_ENVS\n","MINI_BATCH_SIZE = BATCH_SIZE // NUM_MINI_BATCHES\n","NUM_ITERATIONS = TOTAL_STEPS // BATCH_SIZE"]},{"cell_type":"markdown","id":"e3344328452e3250","metadata":{"id":"e3344328452e3250"},"source":["### Task 2: Computing the Advantages\n","\n","The standard advantage formulation is defined as:\n","\n","$$\n","A(s,a) = Q(s,a) - V(s) = r_t + \\gamma V(s') - V(s)\n","$$\n","\n","In Proximal Policy Optimization (PPO), we use Generalized Advantage Estimation (GAE), which is computed as follows:\n","\n","$$\n","\\hat{A}_t = \\delta_t + (\\gamma \\lambda) \\hat{A}_{t+1}\n","$$\n","\n","Expanding this, we have:\n","\n","$$\n","\\hat{A}_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + (\\gamma \\lambda)^2 \\delta_{t+2} + \\ldots + (\\gamma \\lambda)^{T-t-1} \\delta_{T-1}\n","$$\n","\n","where\n","\n","$$\n","\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n","$$\n","\n","The advantage calculation is based on the $TD(\\lambda)$ method, which extends the standard formulation by considering multiple time steps. This allows us to smooth over future rewards using the discount factor $\\gamma$ and a decay factor $\\lambda$ (known as the Generalized Advantage Estimation factor).\n","\n","The essence of GAE is to weigh the TD errors $\\delta_t$ at each time step and accumulate them for a more accurate advantage estimate.\n","\n","In practice, the $TD(\\lambda)$ advantage is computed iteratively, starting with the immediate TD error $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$, and recursively adding future TD errors weighted by $ \\gamma\\lambda$. This approach effectively captures the influence of future rewards in a flexible manner.\n","\n","In this task, you'll be implementing the $\\delta_t$, which is essential for the GAE computation.\n","\n","**Hint**: Remember to use the `next_nonterminal` variable to handle terminal states appropriately."]},{"cell_type":"code","execution_count":null,"id":"dd452162-7de4-4ea9-938c-4020f141c130","metadata":{"id":"dd452162-7de4-4ea9-938c-4020f141c130"},"outputs":[],"source":["def get_deltas(rewards, values, next_values, next_nonterminal, gamma):\n","    \"\"\"Compute the temporal difference (TD) error.\n","\n","    Args:\n","        rewards (torch.Tensor): Rewards at each time step, shape: (batch_size,).\n","        values (torch.Tensor): Predicted values for each state, shape: (batch_size,).\n","        next_values (torch.Tensor): Predicted value for the next state, shape: (batch_size,).\n","        gamma (float): Discount factor.\n","\n","    Returns:\n","        torch.Tensor: Computed TD errors, shape: (batch_size,).\n","    \"\"\"\n","    ### -------------- TASK 2 ------------ ###\n","    ### ----- YOUR CODES START HERE ------ ###\n","    deltas = ?\n","    ### ------ YOUR CODES END HERE ------- ###\n","    return deltas"]},{"cell_type":"code","execution_count":null,"id":"e59cd699-f916-4f6f-8fc1-1dd78d25a680","metadata":{"id":"e59cd699-f916-4f6f-8fc1-1dd78d25a680"},"outputs":[],"source":["# Test get_deltas\n","dummy_rewards = torch.ones(3)\n","dummy_values = torch.tensor([4,5,6])\n","dummy_next_values = torch.arange(3)\n","dummy_next_nonterminal = torch.tensor([1,0,1])\n","dummy_deltas = torch.tensor([-3., -4., -4.8])\n","assert torch.allclose(get_deltas(dummy_rewards, dummy_values, dummy_next_values, dummy_next_nonterminal, gamma=0.1), dummy_deltas)"]},{"cell_type":"markdown","id":"8bc7108f59e92628","metadata":{"id":"8bc7108f59e92628"},"source":["### Task 3: Updating the Actor and Critic Networks\n","\n","After computing the advantages and returns, we shuffle the rollout data and divide the data into mini-batches. We then use the mini-batches to update the actor and critic networks."]},{"cell_type":"markdown","id":"51ff5d30-617d-4f90-808d-7e341d31317d","metadata":{"id":"51ff5d30-617d-4f90-808d-7e341d31317d"},"source":["#### Task 3.1: Compute the Surrogate Policy Objective\n","\n","The policy objective is defined using the following clipped surrogate objective:\n","\n","$$\n","J^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\n","$$\n","\n","Here, $\\hat{A}_t$ is the advantage estimates and $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_\\text{old}}(a_t|s_t)}$ represents the probability ratio between the new policy and the old policy.\n","\n","You will need to implement the `get_ratio` and `get_policy_objective` functions:\n","\n","- **Task 3.1.1**: `get_ratio`: This function should compute the probability ratio $r_t(\\theta)$. To improve numerical stability and avoid issues arising from division by small numbers, rewrite the equation as follows:\n","  $$\n","  r_t(\\theta) = \\exp\\left( \\log\\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_\\text{old}}(a_t|s_t)} \\right) \\right) = \\exp\\left( \\log(\\pi_\\theta(a_t|s_t)) - \\log(\\pi_{\\theta_\\text{old}}(a_t|s_t)) \\right)\n","  $$\n","\n","- **Task 3.1.2**: `get_policy_objective`: This function should compute the value of $J^{CLIP}(\\theta)$ using the results from `get_ratio` and the advantage estimates $\\hat{A}_t$."]},{"cell_type":"code","execution_count":null,"id":"b3b1d476-d8f8-40cc-8e62-5922965f6ee5","metadata":{"id":"b3b1d476-d8f8-40cc-8e62-5922965f6ee5"},"outputs":[],"source":["def get_ratio(logprob, logprob_old):\n","    \"\"\"Compute the probability ratio between the new and old policies.\n","\n","    This function calculates the ratio of the probabilities of actions under\n","    the current policy compared to the old policy, using their logarithmic values.\n","\n","    Args:\n","        logprob (torch.Tensor): Log probability of the action under the current policy,\n","                                shape: (batch_size,).\n","        logprob_old (torch.Tensor): Log probability of the action under the old policy,\n","                                    shape: (batch_size,).\n","\n","    Returns:\n","        torch.Tensor: The probability ratio of the new policy to the old policy,\n","                      shape: (batch_size,).\n","    \"\"\"\n","    ### ------------ TASK 3.1.1 ---------- ###\n","    ### ----- YOUR CODES START HERE ------ ###\n","    logratio = ?  # Compute the log ratio\n","    ratio = ?  # Exponentiate to get the probability ratio\n","    ### ------ YOUR CODES END HERE ------- ###\n","    return ratio"]},{"cell_type":"code","execution_count":null,"id":"38a19e11-ff69-46da-93cc-f43512dc4349","metadata":{"id":"38a19e11-ff69-46da-93cc-f43512dc4349"},"outputs":[],"source":["# Test get_ratio\n","dummy_logprob = torch.tensor([0.1, 0.9])\n","dummy_logprob_old = torch.tensor([0.5, 0.1])\n","dummy_ratio = torch.tensor([0.6703, 2.2255])\n","assert torch.allclose(get_ratio(dummy_logprob, dummy_logprob_old), dummy_ratio, rtol=1e-4)"]},{"cell_type":"code","execution_count":null,"id":"a7cf5fbe-9797-4921-b736-fb93fa973c82","metadata":{"id":"a7cf5fbe-9797-4921-b736-fb93fa973c82"},"outputs":[],"source":["def get_policy_objective(advantages, ratio, clip_coeff=CLIP_COEF):\n","    \"\"\"Compute the clipped surrogate policy objective.\n","\n","    This function calculates the policy objective using the advantages and the\n","    probability ratio, applying clipping to stabilize training.\n","\n","    Args:\n","        advantages (torch.Tensor): The advantage estimates, shape: (batch_size,).\n","        ratio (torch.Tensor): The probability ratio of the new policy to the old policy,\n","                             shape: (batch_size,).\n","        clip_coeff (float, optional): The clipping coefficient for the policy objective.\n","                                       Defaults to CLIP_COEF.\n","\n","    Returns:\n","        torch.Tensor: The computed policy objective, a scalar value.\n","    \"\"\"\n","    ### ------------ TASK 3.1.2 ---------- ###\n","    ### ----- YOUR CODES START HERE ------ ###\n","    policy_objective1 = ?  # Calculate the first policy loss term\n","    policy_objective2 = ?  # Calculate the clipped policy loss term\n","    policy_objective = ?  # Take the minimum and average over the batch\n","    ### ------ YOUR CODES END HERE ------- ###\n","    return policy_objective"]},{"cell_type":"code","execution_count":null,"id":"27064338-dbcb-4181-89c6-5133d2d148dd","metadata":{"id":"27064338-dbcb-4181-89c6-5133d2d148dd"},"outputs":[],"source":["# Test get_policy_objective\n","dummy_advantages = torch.arange(2).float()\n","assert np.allclose(get_policy_objective(dummy_advantages, dummy_ratio).item(), 0.6)"]},{"cell_type":"markdown","id":"b79d7fd2-eb00-419e-8752-cd8594f9ccce","metadata":{"id":"b79d7fd2-eb00-419e-8752-cd8594f9ccce"},"source":["#### Task 3.2: Compute the Value Loss\n","\n","The value loss is calculated as the mean squared error (MSE) between the predicted value and the computed return:\n","\n","$$\n","L^{VF}(\\theta)_{ori} = \\hat{\\mathbb{E}}_t \\left[ \\frac{1}{2} \\left( V(s_t) - {Return}_t \\right)^2 \\right]\n","$$\n","\n","To improve stability during training, we also use a clipped version of the value loss, defined as:\n","\n","$$\n","L^{VF}(\\theta)_{clip} = \\hat{\\mathbb{E}}_t \\left[ \\frac{1}{2} \\left( \\text{clip}(V_{\\theta_t}(s_t), V_{\\theta_{t-1}}(s_t) - \\epsilon, V_{\\theta_{t-1}}(s_t) + \\epsilon) - {Return}_t \\right)^2 \\right]\n","$$\n","\n","It can also be written as:\n","$$\n","L^{VF}(\\theta)_{clip} = \\hat{\\mathbb{E}}_t \\left[ \\frac{1}{2} \\left( V_{\\theta_{t-1}}(s_t) + \\text{clip}(V_{\\theta_t}(s_t) - V_{\\theta_{t-1}}(s_t),  - \\epsilon, + \\epsilon) - {Return}_t \\right)^2 \\right]\n","$$\n","\n","\n","The final value loss is determined by taking the maximum of the two value losses:\n","\n","$$\n","L^{VF}(\\theta) = \\max(L^{VF}(\\theta)_{ori}, L^{VF}(\\theta)_{clip})\n","$$\n","\n","Implement the value loss $L^{VF}(\\theta)$ below."]},{"cell_type":"code","execution_count":null,"id":"a5bbff1c-fcca-432c-a847-0b873411971e","metadata":{"id":"a5bbff1c-fcca-432c-a847-0b873411971e"},"outputs":[],"source":["def get_value_loss(values, values_old, returns):\n","    \"\"\"Compute the combined value loss with clipping.\n","\n","    This function calculates the unclipped and clipped value losses\n","    and returns the maximum of the two to stabilize training.\n","\n","    Args:\n","        values (torch.Tensor): Predicted values from the critic, shape: (batch_size, 1).\n","        values_old (torch.Tensor): Old predicted values from the critic, shape: (batch_size, 1).\n","        returns (torch.Tensor): Computed returns for the corresponding states, shape: (batch_size, 1).\n","\n","    Returns:\n","        torch.Tensor: The combined value loss, a scalar value.\n","    \"\"\"\n","    ### ------------- TASK 3.2 ----------- ###\n","    ### ----- YOUR CODES START HERE ------ ###\n","    value_loss_unclipped = ?  # Calculate unclipped value loss\n","\n","    value_loss_clipped = ?  # Calculate clipped value loss\n","\n","    value_loss = ?  # Average over the batch\n","    ### ------ YOUR CODES END HERE ------- ###\n","    return value_loss  # Return the final combined value loss"]},{"cell_type":"code","execution_count":null,"id":"b4e1ff43-9262-45cf-99d9-f5c09364b7a0","metadata":{"id":"b4e1ff43-9262-45cf-99d9-f5c09364b7a0"},"outputs":[],"source":["# Test get_value_loss\n","dummy_values = torch.tensor([1,2,3]).float()\n","dummy_values_old = torch.tensor([4,5,6]).float()\n","dummy_returns = torch.tensor([7,8,9]).float()\n","assert np.allclose(get_value_loss(dummy_values, dummy_values_old, dummy_returns).item(), 18)"]},{"cell_type":"markdown","id":"bc5e529e-a985-4997-a3c8-c8e34a363b2f","metadata":{"id":"bc5e529e-a985-4997-a3c8-c8e34a363b2f"},"source":["#### Compute Entropy Objective\n","\n","The entropy measures the randomness of the policy's action distribution. By maximizing entropy, we encourage exploration, helping the agent avoid premature convergence to suboptimal strategies. The entropy objective can be computed using the following formula:\n","\n","$$\n","H(\\theta) = - \\mathbb{E}_{s_t} \\left[ \\sum_{a} \\pi_\\theta(a|s_t) \\log(\\pi_\\theta(a|s_t)) \\right]\n","$$\n","\n","The implementation of the entropy objective is given below."]},{"cell_type":"code","execution_count":null,"id":"95d077a8-eb01-42a0-87b4-f4587ddb7c2a","metadata":{"id":"95d077a8-eb01-42a0-87b4-f4587ddb7c2a"},"outputs":[],"source":["def get_entropy_objective(entropy):\n","    \"\"\"Compute the entropy objective.\n","\n","    This function calculates the average entropy of the action distribution,\n","    which encourages exploration by penalizing certainty.\n","\n","    Args:\n","        entropy (torch.Tensor): Entropy values for the action distribution, shape: (batch_size,).\n","\n","    Returns:\n","        torch.Tensor: The computed entropy objective, a scalar value.\n","    \"\"\"\n","    return entropy.mean()  # Return the average entropy"]},{"cell_type":"markdown","id":"c941efe9-56f9-4f4a-a2e6-1f4008bcfb4f","metadata":{"id":"c941efe9-56f9-4f4a-a2e6-1f4008bcfb4f"},"source":["#### Task 3.3: Compute the Total Loss\n","\n","To compute the total loss, we aim to:\n","\n","1. **Maximize the Policy Objective**: This allows the policy to perform better, i.e., gives higher expected returns.\n","\n","2. **Minimize Value Loss**: This helps ensure that the value function accurately estimates future rewards (utility).\n","\n","3. **Maximize Entropy**: This promotes exploration by encouraging the agent to try a variety of actions rather than exploiting known strategies.\n","\n","To implement these objectives using gradient descent, we can transform the maximization of the policy/entropy objective into a minimization problem by minimizing the negative of the objective (i.e., minimizing the policy/entropy loss).\n","\n","In summary, the total loss can be formulated as:\n","\n","$$\n","J^{PPO}(\\theta) = -J^{CLIP}(\\theta) + c_1 L^{VF}(\\theta) - c_2 H(\\theta)\n","$$\n","\n","where $c_1$ is the is the coefficient for value loss (`VALUE_LOSS_COEF`), $c_2$ is the coefficient for entropy loss (`ENTROPY_COEF`) , and $H(\\theta)$ is the entropy of the policy $\\pi_\\theta$."]},{"cell_type":"code","execution_count":null,"id":"917bb6e8-43a8-4af1-8da0-9f132217a8ca","metadata":{"id":"917bb6e8-43a8-4af1-8da0-9f132217a8ca"},"outputs":[],"source":["def get_total_loss(policy_objective, value_loss, entropy_objective, value_loss_coeff=VALUE_LOSS_COEF, entropy_coeff=ENTROPY_COEF):\n","    \"\"\"Compute the total loss for the actor-critic agent.\n","\n","    This function combines the policy objective, value loss, and entropy objective\n","    into a single loss value for optimization. It applies coefficients to scale\n","    the contribution of the value loss and entropy objective.\n","\n","    Args:\n","        policy_objective (torch.Tensor): The policy objective, a scalar value.\n","        value_loss (torch.Tensor): The computed value loss, a scalar value.\n","        entropy_objective (torch.Tensor): The computed entropy objective, a scalar value.\n","        value_loss_coeff (float, optional): Coefficient for scaling the value loss. Defaults to VALUE_LOSS_COEF.\n","        entropy_coeff (float, optional): Coefficient for scaling the entropy loss. Defaults to ENTROPY_COEF.\n","\n","    Returns:\n","        torch.Tensor: The total computed loss, a scalar value.\n","    \"\"\"\n","    ### ------------- TASK 3.3 ----------- ###\n","    ### ----- YOUR CODES START HERE ------ ###\n","    total_loss = ?  # Combine losses\n","    ### ------ YOUR CODES END HERE ------- ###\n","    return total_loss"]},{"cell_type":"code","execution_count":null,"id":"ab496975-855b-4320-b8cc-1ff69ab33d08","metadata":{"id":"ab496975-855b-4320-b8cc-1ff69ab33d08"},"outputs":[],"source":["# Test get_total_loss\n","dummy_policy_objective = torch.tensor(1)\n","dummy_value_loss = torch.tensor(2)\n","dummy_entropy_loss = torch.tensor(3)\n","assert np.allclose(get_total_loss(dummy_policy_objective, dummy_value_loss, dummy_entropy_loss).item(), -0.03)"]},{"cell_type":"markdown","id":"8a2c8587-7c7e-401f-90e5-c51c3362a2e6","metadata":{"id":"8a2c8587-7c7e-401f-90e5-c51c3362a2e6"},"source":["#### (Optional) Running All Checks"]},{"cell_type":"code","execution_count":null,"id":"3b43f187-574c-471c-a2a5-dfb0e1b1ab5f","metadata":{"id":"3b43f187-574c-471c-a2a5-dfb0e1b1ab5f"},"outputs":[],"source":["## Run all checks in the code above again after defining envs, device, ACAgent, showing results.\n","## You can also separate the functions on your own to check on the individual test segments.\n","\n","def run_all_checks(agent, envs, device=\"cpu\"):\n","    \"\"\"\n","    Runs all sanity checks that appear in the notebook/script and prints results.\n","    - Agent interface checks (get_value, get_probs, get_action, get_action_logprob)\n","    - Algorithmic checks: get_deltas, get_ratio, get_policy_objective, get_value_loss, get_total_loss\n","    \"\"\"\n","    import numpy as np\n","    import torch\n","    import torch.nn.functional as F\n","    from torch.distributions import Categorical\n","\n","    torch.set_printoptions(precision=6, sci_mode=True)\n","    # make sampling deterministic (just in case anything stochastic runs)\n","    torch.manual_seed(0)\n","\n","    def _pass(name):\n","        print(f\"✓ {name}\")\n","        return True\n","\n","    def _fail(name, detail):\n","        print(f\"✗ {name}  -->  {detail}\")\n","        return False\n","\n","    ok = True\n","\n","    # ---------- Agent interface checks ----------\n","    try:\n","        obs_dim = envs.single_observation_space.shape[0]\n","        n_actions = envs.single_action_space.n\n","        test_x = torch.zeros(10, obs_dim, device=device, dtype=torch.float32)\n","\n","        # deterministic categorical: each row is a one-hot for a unique action\n","        test_probs = Categorical(\n","            probs=F.one_hot(\n","                torch.arange(0, n_actions, device=device),\n","                num_classes=n_actions\n","            ).float()\n","        )\n","        test_actions = torch.arange(0, n_actions, device=device)\n","        # log(1) == 0; tiny float noise expected\n","        test_logprob = torch.full((n_actions,), -1.1921e-07, device=device)\n","\n","        with torch.no_grad():\n","            v = agent.get_value(test_x)\n","            pr = agent.get_probs(test_x)\n","            sampled = agent.get_action(test_probs)\n","            lps = agent.get_action_logprob(test_probs, test_actions)\n","\n","        print(\"\\n=== Agent interface ===\")\n","        print(\"obs_dim:\", obs_dim, \"  n_actions:\", n_actions)\n","        print(\"values.shape:\", list(v.shape))\n","        print(\"logits.shape:\", list(pr.logits.shape))\n","        print(\"sampled (from one-hot probs):\", sampled.tolist())\n","        print(\"logprob(one-hot actions):\", lps.tolist())\n","\n","        ok &= _pass(\"get_value shape == [10, 1]\") if list(v.shape) == [10, 1] \\\n","              else _fail(\"get_value shape\", f\"got {list(v.shape)}, expected [10, 1]\")\n","\n","        ok &= _pass(\"get_probs logits shape == [10, n_actions]\") if list(pr.logits.shape) == [10, n_actions] \\\n","              else _fail(\"get_probs logits shape\", f\"got {list(pr.logits.shape)}, expected [10, {n_actions}]\")\n","\n","        ok &= _pass(\"get_action(sample from one-hot) == arange(n_actions)\") if torch.equal(sampled, test_actions) \\\n","              else _fail(\"get_action\", f\"got {sampled}, expected {test_actions}\")\n","\n","        ok &= _pass(\"get_action_logprob ≈ zeros (±1e-6)\") if torch.allclose(lps, test_logprob, atol=1e-6) \\\n","              else _fail(\"get_action_logprob\", f\"\\n{lps}\\nvs\\n{test_logprob}\")\n","\n","    except Exception as e:\n","        ok &= _fail(\"Agent interface block crashed\", repr(e))\n","\n","    # ---------- get_deltas ----------\n","    try:\n","        print(\"\\n=== get_deltas ===\")\n","        dummy_rewards = torch.ones(3)\n","        dummy_values = torch.tensor([4., 5., 6.])\n","        dummy_next_values = torch.arange(3, dtype=torch.float32)\n","        dummy_next_nonterminal = torch.tensor([1., 0., 1.])\n","        expected = torch.tensor([-3., -4., -4.8])\n","\n","        out = get_deltas(dummy_rewards, dummy_values, dummy_next_values, dummy_next_nonterminal, gamma=0.1)\n","        print(\"out:\", out.tolist(), \" expected:\", expected.tolist())\n","        ok &= _pass(\"get_deltas matches\") if torch.allclose(out, expected) \\\n","              else _fail(\"get_deltas\", f\"got {out}, expected {expected}\")\n","    except Exception as e:\n","        ok &= _fail(\"get_deltas crashed\", repr(e))\n","\n","    # ---------- get_ratio ----------\n","    try:\n","        print(\"\\n=== get_ratio ===\")\n","        dummy_logprob = torch.tensor([0.1, 0.9])\n","        dummy_logprob_old = torch.tensor([0.5, 0.1])\n","        expected = torch.tensor([0.6703, 2.2255])\n","        out = get_ratio(dummy_logprob, dummy_logprob_old)\n","        print(\"out:\", out.tolist(), \" expected:\", expected.tolist(), \" (rtol=1e-4)\")\n","        ok &= _pass(\"get_ratio matches\") if torch.allclose(out, expected, rtol=1e-4) \\\n","              else _fail(\"get_ratio\", f\"got {out}, expected {expected}\")\n","    except Exception as e:\n","        ok &= _fail(\"get_ratio crashed\", repr(e))\n","\n","    # ---------- get_policy_objective ----------\n","    try:\n","        print(\"\\n=== get_policy_objective ===\")\n","        # Using the ratio we just computed (expected: [0.6703, 2.2255])\n","        advantages = torch.arange(2).float()  # [0., 1.]\n","        out = get_policy_objective(advantages, out)  # reuse 'out' from get_ratio\n","        expected_scalar = 0.6\n","        print(\"out:\", float(out), \" expected:\", expected_scalar)\n","        ok &= _pass(\"get_policy_objective matches\") if np.allclose(float(out), expected_scalar) \\\n","              else _fail(\"get_policy_objective\", f\"got {float(out)}, expected {expected_scalar}\")\n","    except Exception as e:\n","        ok &= _fail(\"get_policy_objective crashed\", repr(e))\n","\n","    # ---------- get_value_loss ----------\n","    try:\n","        print(\"\\n=== get_value_loss ===\")\n","        dummy_values = torch.tensor([1, 2, 3]).float()\n","        dummy_values_old = torch.tensor([4, 5, 6]).float()  # included in signature for clipped loss\n","        dummy_returns = torch.tensor([7, 8, 9]).float()\n","        out = get_value_loss(dummy_values, dummy_values_old, dummy_returns)\n","        expected_scalar = 18.0\n","        print(\"out:\", float(out), \" expected:\", expected_scalar)\n","        ok &= _pass(\"get_value_loss matches\") if np.allclose(float(out), expected_scalar) \\\n","              else _fail(\"get_value_loss\", f\"got {float(out)}, expected {expected_scalar}\")\n","    except Exception as e:\n","        ok &= _fail(\"get_value_loss crashed\", repr(e))\n","\n","    # ---------- get_total_loss ----------\n","    try:\n","        print(\"\\n=== get_total_loss ===\")\n","        dummy_policy_objective = torch.tensor(1.0)\n","        dummy_value_loss = torch.tensor(2.0)\n","        dummy_entropy_loss = torch.tensor(3.0)\n","        out = get_total_loss(dummy_policy_objective, dummy_value_loss, dummy_entropy_loss)\n","        # Uses VALUE_LOSS_COEF=0.5 and ENTROPY_COEF=0.01 :\n","\n","        expected_scalar = -0.03\n","        print(\"out:\", float(out), \" expected:\", expected_scalar)\n","        ok &= _pass(\"get_total_loss matches\") if np.allclose(float(out), expected_scalar) \\\n","              else _fail(\"get_total_loss\", f\"got {float(out)}, expected {expected_scalar}\")\n","    except Exception as e:\n","        ok &= _fail(\"get_total_loss crashed\", repr(e))\n","\n","    print(\"\\n=== Summary ===\")\n","    print(\"ALL CHECKS PASSED\" if ok else \"SOME CHECKS FAILED\")\n","    return ok\n"]},{"cell_type":"code","execution_count":null,"id":"adaf7959-e5d6-44a7-8ec6-ce3b61ac9445","metadata":{"id":"adaf7959-e5d6-44a7-8ec6-ce3b61ac9445"},"outputs":[],"source":["# Run all checks and print out results after you have: envs, device, ACAgent defined\n","run_all_checks(agent, envs, device=device)"]},{"cell_type":"markdown","id":"6a22be9a-58fb-4029-a941-1816117dcf19","metadata":{"id":"6a22be9a-58fb-4029-a941-1816117dcf19"},"source":["#### Training\n","\n","Run the following code to train your agent.\n","\n","**Note:** As a preliminary check, your agent should achieve an episodic total reward of approximately -4000 by the 300th episodes and around -3000 by the 1500th episodes. If this is not the case, it may indicate issues with your implementation."]},{"cell_type":"code","execution_count":null,"id":"20e7083ead820504","metadata":{"id":"20e7083ead820504"},"outputs":[],"source":["# Enable inline plotting\n","%matplotlib inline\n","\n","# Initialize global step counter and reset the environment\n","global_step = 0\n","initial_state, _ = envs.reset()\n","state = torch.Tensor(initial_state).to(device)\n","done = torch.zeros(NUM_ENVS).to(device)\n","\n","# Set up progress tracking\n","progress_bar = tqdm.tqdm(range(1, NUM_ITERATIONS + 1), postfix={'Total Rewards': 0})\n","actor_loss_history = []\n","critic_loss_history = []\n","entropy_objective_history = []\n","\n","reward_history = []\n","episode_history = []\n","\n","# Initialize the optimizer for the agent's parameters\n","optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE, eps=1e-5)\n","\n","for iteration in progress_bar:\n","    # Adjust the learning rate using a linear decay\n","    fraction_completed = 1.0 - (iteration - 1.0) / NUM_ITERATIONS\n","    current_learning_rate = fraction_completed * LEARNING_RATE\n","    optimizer.param_groups[0][\"lr\"] = current_learning_rate\n","\n","    # Perform rollout to gather experience\n","    for step in range(0, ROLLOUT_STEPS):\n","        global_step += NUM_ENVS\n","        states[step] = state\n","        dones[step] = done\n","\n","        with torch.no_grad():\n","            # Get action, log probability, and entropy from the agent\n","            action, log_probability, _ = agent.get_action_logprob_entropy(state)\n","            value = agent.get_value(state)\n","            values[step] = value.flatten()\n","\n","        actions[step] = action\n","        logprobs[step] = log_probability\n","\n","        # Execute action in the environment\n","        # next_state, reward, done, _, info = envs.step(action.cpu().numpy())\n","        next_state, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n","        done = np.logical_or(terminated, truncated)\n","\n","        normalized_reward = (reward - min_reward) / (max_reward - min_reward)  # Normalize the reward\n","        rewards[step] = torch.tensor(normalized_reward).to(device).view(-1)\n","        state = torch.Tensor(next_state).to(device)\n","        done = torch.Tensor(done).to(device)\n","\n","\n","        if \"final_info\" in info:\n","            for i, ep_info in enumerate(info[\"final_info\"]):\n","                if ep_info is not None and \"episode\" in ep_info:\n","                    episodic_reward = ep_info[\"episode\"][\"r\"]\n","                    reward_history.append(episodic_reward)\n","                    episode_history.append(global_step)\n","                    progress_bar.set_postfix({'Total Rewards': episodic_reward})\n","        elif isinstance(info, dict) and \"episode\" in info:\n","            # finished envs mask\n","            mask = info.get(\"_episode\", None)\n","            if mask is None:\n","                mask = info[\"episode\"].get(\"_r\", None)\n","\n","            if mask is not None:\n","                # iterate over the finished envs and log their returns\n","                rs = np.asarray(info[\"episode\"][\"r\"])\n","                for episodic_reward in rs[mask]:\n","                    episodic_reward = float(episodic_reward)\n","                    reward_history.append(episodic_reward)\n","                    episode_history.append(global_step)\n","                    progress_bar.set_postfix({'Total Rewards': episodic_reward})\n","\n","    # Calculate advantages and returns\n","    with torch.no_grad():\n","        next_value = agent.get_value(state).reshape(1, -1)\n","        advantages = torch.zeros_like(rewards).to(device)\n","\n","        last_gae_lambda = 0\n","        for t in reversed(range(ROLLOUT_STEPS)):\n","            if t == ROLLOUT_STEPS - 1:\n","                next_non_terminal = 1.0 - done\n","                next_value = next_value\n","            else:\n","                next_non_terminal = 1.0 - dones[t + 1]\n","                next_value = values[t + 1]\n","\n","            # Compute delta using the utility function\n","            delta = get_deltas(rewards[t], values[t], next_value, next_non_terminal, gamma=GAMMA)\n","\n","            advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_non_terminal * last_gae_lambda\n","        returns = advantages + values\n","\n","    # Flatten the batch data for processing\n","    batch_states = states.reshape((-1,) + envs.single_observation_space.shape)\n","    batch_logprobs = logprobs.reshape(-1)\n","    batch_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n","    batch_advantages = advantages.reshape(-1)\n","    batch_returns = returns.reshape(-1)\n","    batch_values = values.reshape(-1)\n","\n","    # Shuffle the batch data to break correlation between samples\n","    batch_indices = np.arange(BATCH_SIZE)\n","    total_actor_loss = 0\n","    total_critic_loss = 0\n","    total_entropy_objective = 0\n","\n","    for epoch in range(NUM_EPOCHS):\n","        np.random.shuffle(batch_indices)\n","        for start in range(0, BATCH_SIZE, MINI_BATCH_SIZE):\n","            # Get the indices for the mini-batch\n","            end = start + MINI_BATCH_SIZE\n","            mini_batch_indices = batch_indices[start:end]\n","\n","            mini_batch_advantages = batch_advantages[mini_batch_indices]\n","            # Normalize advantages to stabilize training\n","            mini_batch_advantages = (mini_batch_advantages - mini_batch_advantages.mean()) / (mini_batch_advantages.std() + 1e-8)\n","\n","            # Compute new probabilities and values for the mini-batch\n","            new_probabilities = agent.get_probs(batch_states[mini_batch_indices])\n","            new_log_probability = agent.get_action_logprob(new_probabilities, batch_actions.long()[mini_batch_indices])\n","            entropy = agent.get_entropy(new_probabilities)\n","            new_value = agent.get_value(batch_states[mini_batch_indices])\n","\n","            # Calculate the policy loss\n","            ratio = get_ratio(new_log_probability, batch_logprobs[mini_batch_indices])\n","            policy_objective = get_policy_objective(mini_batch_advantages, ratio, clip_coeff=CLIP_COEF)\n","            policy_loss = -policy_objective\n","\n","            # Calculate the value loss\n","            value_loss = get_value_loss(new_value.view(-1), batch_values[mini_batch_indices], batch_returns[mini_batch_indices])\n","\n","            # Calculate the entropy loss\n","            entropy_objective = get_entropy_objective(entropy)\n","\n","            # Combine losses to get the total loss\n","            total_loss = get_total_loss(policy_objective, value_loss, entropy_objective, value_loss_coeff=VALUE_LOSS_COEF, entropy_coeff=ENTROPY_COEF)\n","\n","            optimizer.zero_grad()\n","            total_loss.backward()\n","            # Clip the gradient to stabilize training\n","            nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n","            optimizer.step()\n","\n","            total_actor_loss += policy_loss.item()\n","            total_critic_loss += value_loss.item()\n","            total_entropy_objective += entropy_objective.item()\n","\n","    actor_loss_history.append(total_actor_loss // NUM_EPOCHS)\n","    critic_loss_history.append(total_critic_loss // NUM_EPOCHS)\n","    entropy_objective_history.append(total_entropy_objective // NUM_EPOCHS)\n","\n","    # Prepare data for live plotting\n","    data_to_plot = {\n","        'Total Reward': reward_history,\n","        'Actor Loss': actor_loss_history,\n","        'Critic Loss': critic_loss_history,\n","        'Entropy': entropy_objective_history\n","    }\n","    live_plot(data_to_plot)\n","\n","live_plot(data_to_plot, save_pdf=True, output_file='training_curves.pdf')\n","# Close the environment after training\n","envs.close()"]},{"cell_type":"markdown","id":"38bb3e0c-3a6e-49e9-b88a-4cd80428c3d5","metadata":{"id":"38bb3e0c-3a6e-49e9-b88a-4cd80428c3d5"},"source":["Save the agent model by executing the code below."]},{"cell_type":"code","execution_count":null,"id":"beab51ab-c266-456c-91d5-551bade92d2e","metadata":{"id":"beab51ab-c266-456c-91d5-551bade92d2e"},"outputs":[],"source":["torch.save(agent.state_dict(), \"model.pth\")"]},{"cell_type":"markdown","id":"e28f9142b24f51f5","metadata":{"id":"e28f9142b24f51f5"},"source":["## Agent Evaluation\n","\n","After training the agent, you can evaluate your agent using the following code."]},{"cell_type":"code","execution_count":null,"id":"3cfeb82ca56df783","metadata":{"ExecuteTime":{"end_time":"2024-10-14T14:24:36.513433Z","start_time":"2024-10-14T14:23:27.183068Z"},"id":"3cfeb82ca56df783"},"outputs":[],"source":["env = create_elevator_env()\n","\n","agent_test = ACAgent().to(device)\n","\n","agent_test.load_state_dict(torch.load(\"model.pth\", map_location=device))\n","\n","num_episodes_to_run = 10\n","rewards = []\n","\n","for episode in tqdm.tqdm(range(num_episodes_to_run)):\n","    total_reward = 0\n","\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32, device=device)\n","    while True:\n","        state_tensor = state\n","\n","        with torch.no_grad():\n","            action, _, _ = agent_test.get_action_logprob_entropy(state_tensor)\n","\n","        next_state, reward, terminated, truncated, info = env.step(action.cpu().item())\n","        done = np.logical_or(terminated, truncated)\n","\n","        total_reward += reward\n","\n","        state = torch.tensor(next_state, dtype=torch.float32, device=device)\n","\n","        if done:\n","            break\n","\n","    rewards.append(total_reward)\n","\n","env.close()\n","\n","print(f\"\\nMean Rewards: {np.mean(rewards)}\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":5}